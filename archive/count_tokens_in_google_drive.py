# python3 count_tokens_in_google_drive.py
#
# Requirements (add to requirements.txt if you donâ€™t already have them):
#   pdfminer.six     â€” robust text extraction from PDFs
#   tiktoken         â€” OpenAIâ€‘compatible tokenizer
#
# This script assumes the following helpers already exist, exactly as in
# your other admin utilities:
#   goo_utils.init_auth()
#   goo_utils.get_gcp_clients()         â†’ returns (drive_client, sheets_client)
#   goo_utils.list_pdfs_in_drive_folder(drive_client, folder_id)  # â†’ DataFrame with columns ID, Name
#   goo_utils.download_file_bytes(drive_client, file_id)          # â†’ bytes
#
# If you donâ€™t yet have download_file_bytes, itâ€™s only ~15 lines using
# googleapiclient.http.MediaIoBaseDownload â€“ see comment in code.

import io
import logging
from typing import Tuple

import pandas as pd
import pdfminer.high_level
import tiktoken
from dotenv import load_dotenv

from env_config import *
import gcp_utils as goo_utils

# â”€â”€ Logging & env â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(level=logging.INFO,
                    format="%(asctime)s - %(levelname)s - %(message)s")
load_dotenv(ENV_PATH)

# â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ENCODING = tiktoken.get_encoding("cl100k_base")  # matches GPTâ€‘3.5/4 family

def extract_text_from_pdf(pdf_bytes: bytes) -> str:
    """Return all text found in a PDF (bestâ€‘effort)."""
    with io.BytesIO(pdf_bytes) as fh:
        return pdfminer.high_level.extract_text(fh) or ""

def count_tokens(text: str) -> int:
    """Fast token count using the chosen encoding."""
    return len(ENCODING.encode(text))

def tokens_for_drive_folder(drive_client, folder_id: str) -> Tuple[int, pd.DataFrame]:
    """Download every PDF in a Drive folder and return total + perâ€‘file DataFrame."""
    pdfs_df = goo_utils.list_pdfs_in_drive_folder(drive_client, folder_id)
    total = 0
    per_file_rows = []

    for _, row in pdfs_df.iterrows():
        file_id, name = row["ID"], row["Name"]
        try:
            fh = goo_utils.fetch_pdf_from_drive(drive_client, file_id)
            if fh is None:
                raise RuntimeError("Download failed")
            text = pdfminer.high_level.extract_text(fh)
            n_tokens = count_tokens(text)
            total += n_tokens
            per_file_rows.append({"file_name": name, "file_id": file_id, "tokens": n_tokens})
            logging.info(f"{name}: {n_tokens:,}â€¯tokens")
        except Exception as e:
            logging.error(f"âŒÂ {name}: {e}")

    return total, pd.DataFrame(per_file_rows)


def main():
    logging.info("ğŸ”  Counting tokens in Drive PDFs â€¦")

    goo_utils.init_auth()
    drive_client, _ = goo_utils.get_gcp_clients()

    backlog_total, backlog_df = tokens_for_drive_folder(drive_client, PDF_TAGGING_FOLDER)
    live_total,    live_df    = tokens_for_drive_folder(drive_client, PDF_LIVE_FOLDER_ID)

    grand_total = backlog_total + live_total

    logging.info(f"ğŸ“‚ Backlog folder: {backlog_total:,}â€¯tokens")
    logging.info(f"ğŸ“‚ Live    folder: {live_total:,}â€¯tokens")
    logging.info(f"ğŸ Grand total:    {grand_total:,}â€¯tokens")

    (pd.concat([backlog_df.assign(folder="Backlog"),
                live_df.assign(folder="Live")], ignore_index=True)
        .to_csv("pdf_token_counts.csv", index=False))
    logging.info("ğŸ“  Detailed breakdown written to pdf_token_counts.csv")

if __name__ == "__main__":
    main()
